{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Blocks in HEP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BB Motivation Outline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Choose optimal bin edges for a given histogram\n",
    "    1. Bin edges should convey meaning\n",
    "    2. Content in each bin should be self-similar\n",
    "    3. Edge choice should be unbiased\n",
    "2. Decide how optimal edges are chosen\n",
    "    1. Assume all data within a bin is consistent with a single pdf\n",
    "    2. Variation is solely due to statistical fluctuation\n",
    "    3. Default case: each bin is modeled by uniform pdf\n",
    "    4. Additional criterion:\n",
    "        1. There should be a penalty for more bins\n",
    "3. Benefits:\n",
    "    1. Visual:\n",
    "        1. Removes user bias\n",
    "        2. Not bound by fixed-width\n",
    "        3. Can cover large changes in count, orders of mag\n",
    "        4. No empty bins (good for particle physics)\n",
    "        5. No wild variations in ratio plot\n",
    "        6. Can make signal easy to spot\n",
    "    2. Statistical:\n",
    "        1. Natural way to do binned shape analysis?\n",
    "        2. Some sort of bias-variance tradeoff?\n",
    "        3. Remove some statistical variation, but also remove some shape info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hgg Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can BB be used for a classic bump hunt?\n",
    "\n",
    "1. Most simplistic case: no signal model, no bg model, simply \"look\" for a bump\n",
    "    1. Algo in its current state cannot locate peaks on a falling background\n",
    "    2. May require change in underlying pdf assumptions\n",
    "2. More typical case: signal model and bg model exist\n",
    "    1. Apply BB on bg and signal model independently\n",
    "    2. Combine bin edges to create a hybrid binning\n",
    "    3. Apply hybrid binning to data\n",
    "        1. Is hybrid binning more visually appealing than a standard binning?\n",
    "        2. Does the hybrid binning have comparable statistical power to an unbinned likelihood?\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![black_hole_1](figures/ST_mul8_mc_and_data_normed_databin_signal_shape.png)\n",
    "![black_hole_2](figures/ST_mul8_mc_and_data_normed_databin_nobb_signal_shape.png)\n",
    "![DY comparison](figures/z_data_hist_binsVbb.png)\n",
    "![DY_ratio1](figures/bb_Z_gen_reco.png)\n",
    "![DY_ratio2](figures/b25_Z_gen_reco.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Explanation of Bayesian Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian Blocks is a nonparametric modeling technique for determining the optimal segmentation of a given data set.  The optimal segmentation presented here is one that maximizes an expression that models the data a series of piecewise constant values.  This expression is referred to as a 'fitness function', which is constructed from the unbinned likelihood known as the Cash Statistic.  Each segment is equivalent to a histogram bin, where the data contained in each bin are consistent with a Poisson rate for that bin.  Therefore, the bin edges are statistically significant, because they denote a change in expected event rate for a group of data.  To prevent the extremum case in which each data point is contained in its own bin, a regularization parameter is applied to penalize the fitness function as the number of bins grows.\n",
    "\n",
    "This analyses takes advantage of the optimization algorithm presented in [Scargle], which guarantees that the global maximum for the fitness function is found for any given set of data.  The regularization parameter must be determined empirically for any given dataset.  Following the work performed in [Scargle], we use a parameter equivalent to a false-positive rate of 1%, which sufficiently suppresses non-monotonic behavior when applied to monotonically increasing or decreasing datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: A Classic Bump Hunt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bayesian Block technique was developed with the Astronomy community in mind.  The data are typically time-series photon counts, where the background and signals are well-described by uniform pdfs.  This is rarely the case in particle physics; the data is typically time-independent and follows a smoothly increasing or decreasing spectrum.  In the context of  reconstructed invariant mass, a signal can usually be modeled as a Gaussian-like peak on top of a rising or falling spectrum. \n",
    "\n",
    "The naive application of the Bayesian Block algorithm leads to..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
